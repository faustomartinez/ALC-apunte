\chapter{Sistemas y Factorización de Matrices}

Comenzaremos asumiendo que trabajaremos con matrices cuadradas, i.e.  $\Ab\in \R^{n\times n}$ (o mas en general
$\C^{n\times n}$ ya que la mayoría de las consideranciones se aplican a los complejos).

Los vectores $\vb\in \Cn$ los identificamos con matrices columna de $n\times 1$ de donde tiene sentido $\Ab\vb$.

Factorizar siginifica descomponer una expresión en factores de algún modo mas simples o que nos otorguen algún beneficio teórico o práctico. La factorización
LU consiste en escribir una matriz invertible $\Ab$ como producto de dos factores: uno triangular inferior $\Lb$ y otro triangular superior $\Ub$. Un beneficio inmediato de esta factorización es que para  resolver un sistema
$$
\Ab\xb=\bb,
$$
basta resolver
$$
\Lb\yb=\bb,
$$
$$
\Ub\xb=\yb,
$$
y cada sistema triangular puede resolverse por sustitución (regresiva o progresiva) en $\sim n^2$ operaciones
\begin{ej}Resolución sistemas triangulares: dar un algoritmo para resolverlo en orden $n^2$
\end{ej}
\begin{ej}Resolución sistemas tridiagonales: dar un algoritmo para resolverlo en orden $n$:
{\bf Motivar con ecuaciones diferenciales-}
\end{ej}

\section{Descomposición $\Lb\Ub=\Ab$}

La factorización $LU$ es un subproducto del método de eliminación de Gauss.
Dada la matriz
$$
\Ab= \begin{pmatrix}
a_{1,1}& a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n,1} & a_{n,2} & \cdots & a_{n,n}
\end{pmatrix}
$$
y suponiendo que $a_{1,1}$, denominado \emph{pivot}, es no nulo es posible operar
sobre las filas de $\Ab$ para generar ceros debajo del elemento
$a_{1,1}$. Esto se hace fila a fila multiplicando la fila 1 de $\Ab$ por $-\frac{a_{i,1}}{a_{1,1}}$ (lo cual es
posible ya que  $a_{1,1}\neq 0$) y sumando ese resultado a
la fila $i$. Para la fila 1, en términos matriciales, eso
equivale a multiplicar a izquierda la matriz $\Ab$ por el
%multiplicador $\Mb_1$
$$
\Mb_1 =
\begin{pmatrix}
1 & 0 & \cdots & 0 \\
-\frac{a_{2,1}}{a_{1,1}} & 1 & \cdots & 0 \\
\vdots  & \vdots  & \ddots & \vdots  \\
-\frac{a_{n,1}}{a_{1,1}} & 0 & \cdots & 1
\end{pmatrix} $$
El efecto neto del producto resulta
$$
\Mb_1\Ab= \begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} &\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} &\cdots & a^{(1)}_{2,n} \\
0 & a^{(1)}_{3,2} & a^{(1)}_{3,3} & \cdots & a^{(1)}_{3,n} \\

\vdots  & \vdots  & \ddots & \vdots  \\
0 & a^{(1)}_{n,2}& a^{(1)}_{n,3} & \cdots & a^{(1)}_{n,n}
\end{pmatrix}
$$
donde se destaca con un superíndice $(1)$ una nueva matriz
$\Ab^{(1)}\in \R^{(n-1)\times (n-1)} $
$$
\Ab^{(1)}=
 \begin{pmatrix}
 a^{(1)}_{2,2} & \cdots & a^{(1)}_{2,n} \\
\vdots  & \vdots  & \ddots &   \\
a^{(1)}_{n,2} & \cdots & a^{(1)}_{n,n}
\end{pmatrix}
$$
que en caso de tener el pivot $a^{(1)}_{2,2}\neq 0$ admite un nuevo paso en la iteración tomando
$$
\Mb_2 =
\begin{pmatrix}
1 & 0 & \cdots & 0 & 0 \\
0 & 1& \cdots & 0 & 0 \\
0 & -\frac{a_{3,2}^{(1)}}{a^{(1)}_{2,2}} & 1 & \cdots & 0 \\
\vdots  & \vdots  & \vdots & \vdots  \\
0&-\frac{a_{n,2}^{(1)}}{a^{(1)}_{2,2}} & 0 & \cdots & 1
\end{pmatrix} $$
de donde
$$
\Mb_2\Mb_1\Ab= \begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} &\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} &\cdots & a^{(1)}_{2,n} \\
0 & 0 & a^{(2)}_{3,3} & \cdots & a^{(2)}_{3,n} \\

\vdots  & \vdots  & \ddots & \vdots  \\
0 & 0& a^{(2)}_{n,3} & \cdots & a^{(2)}_{n,n}
\end{pmatrix}
=\begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} &\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} &\cdots & a^{(1)}_{2,n} \\
0 & 0 & &  &  \\
\vdots  & \vdots &  & \Ab^{(2)} &   \\
0 & 0&  &
\end{pmatrix},
$$
con $\Ab^{(2)}\in \R^{(n-2)\times (n-2)}$.
Este procedimiento, dado que cada $\Mb_i$ es triangular, se denomina  \emph{triangulación por matrices triangulares}. Si algoritmo no se detiene, es decir que cada elemento pivot $a_{k+1,k+1}^{(k)}\neq 0$, para todo $1\le k\le n-1$ (notar que el último pivot puede ser nulo sin impedir la finalización del algoritmo), se obtiene una matriz triangular superior $\Ub$
$$
\Mb_{n-1}\Mb_{n-2}\cdots \Mb_{1}\Ab=\Ub=
\begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} & a_{1,4}&\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} & a^{(1)}_{2,4} &\cdots & a^{(1)}_{2,n} \\
0 &0 & a^{(2)}_{3,3} & a^{(2)}_{3,4}  &\cdots & a^{(2)}_{3,n} \\
%|0 & 0 & 0 &  & & \\
\vdots  & \vdots & \vdots  & \vdots  & \vdots  & \vdots   \\
0 & 0& 0 & 0& 0& a^{(n-1)}_{n,n}
\end{pmatrix}
$$
Observemos que la productoria
$$
\Mb_{n-1}\cdots\Mb_2\Mb_1=\Mb,
$$
es una matriz triangular inferior, dado que es producto de matrices triangulares inferiores. La inversa de $\Mb$, que existe sí y solo sí $a_{k,k}^{(k-1)}\neq 0$ $\forall k$, $1\le k\le n$, es por lo tanto triangular inferior. La llamaremos $\Lb=\Mb^{-1}$ y por ende
$$
\Ab=\Lb\Ub.
$$
El problema que aparece inmediatamente es que no hemos obtenido $\Lb$ constructivamente. Sin embargo, un hecho notable facilita ese trabajo.

Observemos que cada $\Mb_i$ tiene la forma
\begin{equation}
 \label{eq:formaMi}
\Mb_i=\Ib-\zb_i\eb_i^T,
\end{equation}
donde
$\eb_i$ representa el $i-$ésimo canónico y
$\zb_i$ es de la forma
$$
\zb_i=
\begin{pmatrix}
 0\\
 \vdots\\
 0\\
 z_{i+1}^{(i)}\\
 \vdots \\
 z_n^{(i)}
\end{pmatrix},
$$
con $z_j^{(i)}=\frac{a^{(i-1)}_{j,i}}{a^{(i-1)}_{i,i}}$, $i+1\le j\le n$.

\begin{tcolorbox}[colback=black!15!white,colframe=black!75!black]
\begin{lema}\label{lema:inversamultiplicador}
 Sea $\ub,\vb\in \Rn$, tales que $\vb^T\ub\neq 1$ entonces $\Ib-\ub\vb^T$ es invertible y además
 $$(\Ib-\ub\vb^T)^{-1}=\Ib+\frac{\ub\vb^T}{1-\vb^T\ub}.$$
\end{lema}
\end{tcolorbox}
\begin{proof}
 $$(\Ib-\ub\vb^T)(\Ib+\frac{\ub\vb^T}{1-\vb^T\ub})=\Ib+\ub\vb^T(\frac{1}{1-\vb^T\ub}-1)-\frac{\ub\vb^T\ub\vb^T}{1-\vb^T\ub}=\Ib. $$
\end{proof}
Considerando el lema anterior y observando que en  \eqref{eq:formaMi} se tiene
$\eb_i^T\zb_i=0$, resulta \footnote{Es decir que el costo de invertir $\Mb_i$ es casi nulo (apenas un cambio de signos debajo de la diagonal).}
$$
\Mb_i^{-1}=\Ib+\zb_i\eb_i^T,
$$
de donde
$$
\Lb=\Mb_1^{-1}\Mb_2^{-1}\cdots \Mb_{(n-1)}^{-1}=(\Ib+\zb_1\eb_1^T)(\Ib+\zb_2\eb_2^T)\cdots (\Ib+\zb_{n-1}\eb_{n-1}^T).
$$
Ademas de la sencillez en el cómputo de $\Mb_i^{-1}$ hay otro hecho ``afortunado'' que permite expresar $\Lb$ sin cálculos adicionales. Como $\eb_i^T\zb_k=0$ no solo para $k=i$ sino para todo $i\le k\le n$,  se puede desarrollar la expresión para $\Lb$ y obtener
$$
\Lb=\Ib+\sum_{i=1}^{n-1}\zb_i\eb_i^T,
$$
o dicho de otra forma,
$$
\Lb=
\begin{pmatrix}
1 & 0 & \cdots & 0 & 0\\
\frac{a_{2,1}}{a_{1,1}} & 1 & \cdots & 0 & 0 \\
\frac{a_{3,1}}{a_{1,1}} & \frac{a_{3,2}^{(1)}}{a^{(1)}_{2,2}}
\vdots  & \vdots  & \ddots & \vdots  \\
\vdots  & \vdots  & \ddots &1&  \vdots  \\
\frac{a_{n,1}}{a_{1,1}} &  \frac{a_{n,2}^{(1)}}{a^{(1)}_{2,2}} & \cdots & \frac{a_{n,n-1}^{(n-1)}}{a^{(n-1)}_{n-1,n-1}}& 1
\end{pmatrix},
$$
i.e., basta con almacenar los multiplicadores en sus respectivos lugares cambiando los signos.




Un supuesto básico para que la descomposición LU pueda llevarse a cabo
es que todos los pivots sean no nulos.
Eso no puede garantizarse bajo la única hipótesis de que $det(\Ab)\neq 0$ como se ve en tomando por ejemplo
$$
\Ab=
\begin{pmatrix}
 0&1\\
 1&0
\end{pmatrix}
$$

Sin embargo puede garantizarse si se requiere que todos lo menores de la matriz $\Ab$,
sean invertibles.

\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black]
\begin{prop}\label{prop:menorenonulos}
 Sea $\Ab\in \Rnn$, entonces existe factorización LU de $\Ab$ con $\Lb,\Ub$ \emph{invertibles} sí y solo sí
 para todo  $1\le k\le n$,  $det(\Ab(1:k,1:k))\neq 0$.
\end{prop}

\end{tcolorbox}
 \begin{proof}
  hacer...
 \end{proof}

Para ver otro criterio necesitamos una definición.
\tccdefi
\begin{defi}
  \label{defi:deDD}
  Una matriz $\Ab\in \Rnn, \Cnn$ se dice \emph{diagonal dominante} (\emph{estrictamente diagonal dominante}) y se denota DD (EDD) si y solo si para todo $i$,
 $1\le i\le n$
 $$
 \sum_{1\le j \le n, j\neq i}|a_{i,j}|\le (<) |a_{i,i}|.
 $$

 \end{defi}
\end{tcolorbox}
Y la siguiente proposición (que puede refinarse en algunos casos como veremos mas adelante).
\tcc
\begin{prop}
 Sea $\Ab\in \Rnn, \Cnn$ EDD entonces $\Ab$ es invertible.
\end{prop}
\etcc
\begin{proof}
 Supongamos que no. Existe entonces
 ${\bf 0}\neq \vb\in\Cn$ tal que
 $\Ab\vb=\cero$. Sea $i$, $1\le i\le n$ tal que $0\neq |v_i|=\|\vb\|_\infty$, entonces
 $$
 \sum_{j=1}^na_{i,j}v_j=0,
 $$
 de donde
 $$
 |a_{i,i}|\le \sum_{j=1, j\neq i}^n|a_{i,j}|\frac{|v_j|}{|v_i|}\le
 \sum_{j=1, j\neq i}^n|a_{i,j}|,
 $$
 lo que contradice la EDD.
\end{proof}






\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black] \begin{prop}
  Si $\Ab$ es EDD entonces, trabajando con aritmética exacta, el proceso de eliminación de Gauss no produce pivots nulos.
 \end{prop}
\end{tcolorbox}
\begin{proof}
 hacer...
\end{proof}

\begin{tcolorbox}
\begin{rem}
 Las matrices DD no son una artificio teórico, aparecen frecuentemente en la práctica. En particular en la discretización de ecuaciones diferenciales, como veremos en algún ejemplo mas adelante.

 Citaría el caso de las tridigonales clásicas y las de dos variables...
 observaría que no es EDD y agregaría algo de la teoría del libro de Ortega.
\end{rem}
\etcc

Aún en el caso en que no aparezcan pivots nulos, el algoritmo de eliminación no delvoverá en general los verdaderos factores $\Lb$ y $\Ub$, sino una aproximación de ellos. Mas explícitamente podemos enunciar el siguiente teorema.
\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black]
\begin{teo}\label{teo:errorLU}
 Sea $\Ab\in \Rnn$, y $\tilde\Ab=fl(\Ab)$, si el algoritmo LU no produce pivots nulos,la factorización LU producida por la máquina verifica
 $$
 \tilde\Lb\tilde \Ub=\tilde \Ab + \Hb
 $$
donde $|\Hb|\lesssim 2(n-1)\mu(|\tilde \Ab|+|\tilde \Lb||\tilde \Ub|)
+\mathcal{O}(\mu^2).
$
\end{teo}

\end{tcolorbox}
\begin{proof}
 hacer ... no es larga.
\end{proof}


\begin{tcolorbox}
\begin{rem}
 El Teorema \ref{teo:errorLU}, sugiere controlar el tamaño de los factores
$\tilde\Lb,\tilde\Ub$. En efecto, si el producto
$|\tilde \Lb||\tilde \Ub|$ es muy grande no tendremos control (al menos teórico) sobre el error. El \emph{pivoteo parcial} permite garantizar al menos el control $|\Lb|\le 1$ a una costo razonable. Si bien, esto no necesariamente implica que $|\tilde \Lb||\tilde \Ub|$ sea pequeño, resulta suficiente en la mayoría de los casos prácticos (ver sin embargo la Observación \ref{obs:malgauss}).

\end{rem}

\end{tcolorbox}


\begin{ej}
 Muestre el el número de operaciones necesarias para realizar la factorización LU es $\mathcal{O} (\frac{2n^3}{3})$
\end{ej}

El número de elementos de una matriz de $n\times n$ es obviamente $n^2$. El número mínimo de operaciones que un método de resolución puede efectuar debe ser de orden mayor o igual a $n^2$. Todos los métodos directos clásicos son de orden cúbico. Esto presenta problemas para matrices muy grandes ya que en las aplicaciones es posible tener mas de $10^6$ incógnitas.

Otro problema importante es el \emph{rellenado} que produce el algoritmo. Esto es, aunque $\Ab$ sea \emph{rala} -cosa que afortunadamente ocurre en muchas aplicaciones, como en ecuaciones diferenciales- los factores $\Lb,\Ub$ pueden no serlo.

\begin{ej}
 Cuanta memoria ocuparía una matriz
 de $10^6\times 10^6$ llena?.
\end{ej}



\section{Descomposición $\Lb\Ub=\Pb\Ab$ (pivoteo parcial)}
Una matriz de permutaciones $\Pb\in \Rnn$
es una matriz identidad con las filas permutadas. Observemos las siguientes propiedades de las permutaciones.
\begin{itemize}
 \item Dada $\Ab\in \Rnn$, $\Pb\Ab$ coincide con $\Ab$ pero con las filas permutadas y $\Ab\Pb$ coincide con $\Ab$ pero con las columnas permutadas.
 \item Dada $\xb\in \Rn$, $\Pb\xb$ coincide con $\xb$ pero con los elementos permutados.
 \item $det(\Pb)=\pm 1$
 \item $\Pb^{-1}=\Pb^T$
 \item Si $\Pb_1$ y $\Pb_2$ son permutaciones entonces $\Pb=\Pb_1\Pb_2$ también lo es.
\end{itemize}
\begin{tcolorbox}
Esto es para poner mas adelante---
 \begin{rem}
 \begin{enumerate}
  \item Las matrices de Markov (también llamadas estocásticas) verifican que $0\le a_{i,j}\le 1$
  y $\sum_{i=1}^na_{i,j}=1$. Es decir que cada columna puede interpretarse como una distribución discreta de probabilidad (mas adelante retomaremos este tema).
  \item Una matriz es bi-estocástica si tanto $\Ab$ como $\Ab^T$ son de Markov. Puede probarse que toda matriz de este tipo es combinación convexa de matrices de permutaciones
  (Teorema de Brikhoff-Von Neumann).
 \end{enumerate}
 \end{rem}
\end{tcolorbox}


Si durante la eliminación de Gauss hallamos un pivot nulo, podemos intercambiar filas para continuar con el algoritmo (siempre que haya algún elemento no nulo con el cual proseguir). Mas aún, aunque el correspondiente pivot sea no nulo, podemos,  casi al mismo costo, tomar el pivot mas grande (en valor absoluto) para  garantizar que $|\tilde\Lb|\le 1$.  Esa operación la podemos representar matricialmente multiplicando por una adecuada permutación.

El procedimiento sería el siguiente: tomemos el elemento con mayor valor absoluto en la primera columna de $\Ab$. Digamos que este es $a_{k,1}$ (si $det(\Ab)\neq 0$ se tiene que $a_{k,1}\neq 0$). Permutemos la fila $k$ con la fila $1$, lo cual se hace con una matriz $\Pb_1$. Es decir
$$
\Pb_1\Ab=
\begin{pmatrix}
 a_{k,1}&a_{k,2}&\cdots &a_{k,n}\\
 a_{2,1}&a_{2,2}&\cdots &a_{2,n}\\
  \vdots&\vdots&\vdots&\vdots\\
 a_{1,1}&a_{1,2}&\cdots &a_{1,n}\\
 \vdots&\vdots&\vdots&\vdots \\
 a_{n,1}&a_{n,2}&\cdots &a_{n,n}
\end{pmatrix},
$$
a partir de aquí construimos el multiplicador, $\Mb_1$ de modo tal que
$$
\Mb_1\Pb_1\Ab=
\begin{pmatrix}
 a_{k,1}&a_{k,2}&\cdots &a_{k,n}\\
 0&&&\\
  \vdots&&\Ab^{(1)}&\\
 0&&&\\

\end{pmatrix}.
$$
Llegado a este punto, repetimos el procedimiento con la primer columna de $\Ab^{(1)}$ que eventualmente requerirá otra matriz $\Pb_2$ antes de la construcción del nuevo multiplicador $\Mb_2$. En definitiva,
\begin{equation}
 \label{eq:factoresPLU}
\Mb_{n-1}\Pb_{n-1}\cdots \Mb_{1}\Pb_{1}\Ab=\Ub,
 \end{equation}


se obtiene una nueva matriz triangular superior $\Ub$, y donde
$\Mb_i=\Ib-\zb_i\eb_i^T$ con
$$
\zb_i=
\begin{pmatrix}
 0\\
 \vdots\\
 0\\
 z_{i+1}^{(i)}\\
 \vdots \\
 z_n^{(i)}
\end{pmatrix},
$$


$|\zb_i|\le 1$ (i.e. los multplicadores sin menores o iguales a 1 en valor absoluto).
El problema ahora es que en la forma \eqref{eq:factoresPLU}  no parece verse el modo de reconstruir los factores $\Lb$ y $\Pb$ de forma sencilla. Sin embargo otro hecho inesperadamente favorable resuelve esta cuestión. Notemos que por el orden de las iteraciones en el paso $k$ solo permutaremos en busca del mayor pivot (en caso de ser necesario) filas desde la $k$ a la $n$ porque las filas previas ya han sido procesadas. Eso indica que
$\eb_j^T\Pb_k=\eb^T_j$ para todo $j<k$. En particular, si bien \emph{no es cierto} que $\Pb_k$
conmuta con $\Mb_j$ sí es cierto,
\emph{para todo $j<k$}, que
$$
\Pb_k\Mb_j=\Pb_k
(\Ib-\zb_{j}\eb_{j}^T)=
(\Ib-\Pb_k\zb_{j}\eb_{j}^T)\Pb_k=
\tilde\Mb_j \Pb_k,
$$
es decir que podemos pasar la permutación a la derecha permutando adecuadamente los multiplicadores de la matriz $\Mb_j$. Lo importante es que esta nueva matriz, denotada con $\tilde \Mb_j$ tiene la misma estructura que $\Mb_j$. Así que volviendo a  \eqref{eq:factoresPLU} tenemos
$$
\Mb_{n-1}\Pb_{n-1}\cdots \Mb_{1}\Pb_{1}\Ab= \tilde \Mb_{n-1}\cdots \tilde \Mb_{1} \Pb_{n-1}\cdots \Pb_{1}\Ab=\Ub.
$$
Argumentando como en el caso son pivoteo (usando que la estructura de las $\tilde \Mb_i$ es igual a la de las de las $\Mb_i$) y llamando
$\Pb=\Pb_{n-1}\cdots \Pb_{1}$ se llega a
$$
\Pb\Ab=\Lb\Ub.
$$

\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black] \begin{prop}
  Trabajando con aritmética exacta se tiene que si $det(\Ab)\neq 0$ el algoritmo de pivoteo parcial no tiene pivots nulos.
 \end{prop}
\end{tcolorbox}

\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black] \begin{rem}
\label{obs:malgauss}
El proceso de pivoteo parcial garaniza que $|\Lb|\le 1$ pero no
se un tiene control demasiado beningno sobre el tamaño de
$|\Ub|$, como se ve en este ejemplo
$$
\begin{pmatrix}
 1&0&0&\cdots&0&1\\
 -1&1&0&\cdots&0&1\\
 -1&0&1&0&\cdots&1 \\
 \vdots &\vdots&\vdots& \ddots&&\vdots\\
 -1&0&0&0&\cdots&1
\end{pmatrix},
$$
puede verse que $\|\Ub\|_\infty \ge 2^n\|\Ab\|_\infty$.
Este ejemplo sin embargo parece ser singular en el sentido de que en los casos genéricos el crecimiento de $|\Ub|$ estaría controlado por $\sqrt{n}$ \cite{TB} \footnote{Al presente no parece haber una demostración de esto aunque Trefethen ofrece un premio de 200 dólares por una. Buscar el SIAM news donde esta esto.} Esta particularidad parece (junto con el hecho de que su costo es la mitad que el de la factorización $QR$ que veremos mas adelante) haber mantenido el algoritmo de eliminación como uno de los más populares entre los métodos directos.
 \end{rem}
\end{tcolorbox}

Controlar el tamaño del ambos factores $\Lb$ y $\Ub$ es posible.
La idea se denomina \emph{pivoteo completo}, esto implica hacer permutaciones de filas y de columnas para tomar siempre el mayor pivot posible. Así en el paso inicial se busca el elemento $a_{k,l}$ con máximo valor absoluto y se permutan la fila y columna 1 con la fila y columna $k$ y $l$ respectivamente.  En términos matriciales eso requiere de dos matrices de permutaciones $\Pb_1,\tilde \Pb_1$ antes de aplicar el paso de eliminación. Es decir
$$
\Mb_1\Pb_1\Ab\tilde\Pb_1=
\begin{pmatrix}
 *&*&\cdots &*\\
 0&&&\\
  \vdots&&\Ab^{(1)}&\\
 0&&&\\

\end{pmatrix}.
$$
Repitiendo el procedimiento se obtiene $\Ub$
$$
\Mb_{n-1}\Pb_{n-1}\cdots \Mb_1\Pb_1\Ab\tilde\Pb_1\cdots \tilde\Pb_{n-1}=\Ub,
$$
y argumetando como antes se obtienen ahora dos matrices de permutaciones $\Pb,\tilde\Pb$ tales que
$$
\Pb\Ab\tilde\Pb=\Lb\Ub.
$$
El pivoteo completo no se lleva a cabo en general debido a su alto costo.  (poner el costo... suponiendo que las comparaciones valen por operaciones tradicionales). Ya que hay otros métodos estables mas económicos. Wilkinson probó lo siguiente
\tcc
\begin{teo}
 En aritmética exacta, para el algoritmo de pivoteo completo se tiene
 $$
 \|\Ub\|_{\infty}\le \sqrt{n}(2.3^{1/2}.4^{1/3}\cdots n^{1/(n-1)})^{1/2}\|\Ab\|_\infty
 $$
\end{teo}
\etcc
\begin{ej}
 Estimar el crecimiento del factor
 $\sqrt{n}(2.3^{1/2}.4^{1/3}\cdots n^{1/(n-1)})^{1/2}$.
\end{ej}

\begin{itemize}
 \item falta hablar de unicidad de LU
\end{itemize}

\section{Descomposición $\Lb \Db\Lb^*$ : Cholesky}
Voy a poner aca Cholesky porque no preciso demasiadas propiedades las matrices  simetricas.. me basta definicion de eso y definida positiva. En todo caso si falta algo lo citamos y lo damos mas adelante. De otro modo el metodo nos va a quedar muy descolgado mas adelante... (creo...)

En muchos casos es posible explotar la estructura de las matrices a la hora de resolver sistemas. Dicho sea esto a la hora de ahorrar computos o mejorar la estabilidad de los algoritmos.

\begin{tcolorbox}
 \begin{defi}
 Una matriz $\Ab\in \Cnn$ se dice \emph{Hermitiana} si $\Ab=\Ab^*$. En el caso de que $\Ab\in \Rnn$, $\Ab$ se dice \emph{simétrica}.
\end{defi}

\begin{defi}
 Una matriz $\Ab\in \Cnn$ ($\Ab\in \Rnn$)  se dice \emph{definida positiva} si es Hermitiana (simétrica) y además $\vb^*\Ab\vb>0$ para todo $\vb\neq 0$.
\end{defi}
\end{tcolorbox}
Notar en la Definición anterior que
$\Ab$ Hermitiana, implica que $\vb^*\Ab\vb\in \R$.

\begin{prop}
\label{prop:defposesinvert}
Si $\Ab$ es definida positiva entonces es invertible y también lo son todos sus menores.
\end{prop}
\begin{proof}
 En efecto, de no ser invertible, existiría $\cero\neq\vb$ tal que
 $\Ab\vb=\cero$ lo que contradice la definida positividad ya que para ese $\vb$ se tendría $\vb^*\Ab\vb=0$.

 Respecto de los menores el resultado se sigue del anterior. En efecto, sea $k$ fijo, $1\le k\le n$ y tomemos
 vectores $\vb\in \C^n$ de la forma
 $(\tilde \vb,0,\cdots,0)$ con $\tilde \vb \in \C^k$ arbitrario. Resulta
 $$
 0<\vb^*\Ab\vb=\tilde\vb^*\Ab(1:k,1:k)\tilde \vb,
 $$
 y de ahí la definida positividad de $\Ab(1:k,1:k)$ y por ende su invertibilidad.
\end{proof}


Si $\Ab$ es definida positiva  admite una descomposición $\Ab=\Lb\Ub$ (gracias a las Proposiciones \ref{prop:defposesinvert} y \ref{prop:menorenonulos}) y
podemos escribir  $\Ub=\Db\tilde \Lb^*$, con $\tilde \Lb$ triangular inferior con $1$ en la diagonal y $\Db$ una matriz diagonal. De ahí resulta
$$
\Ab=\Lb\Db\tilde{ \Lb}^*=\tilde \Lb\Db^*\Lb^*,
$$
entonces
$$
\Db(\Lb^{-1}\tilde{ \Lb})^* = \Lb^{-1}\tilde \Lb\Db^*,
$$
y como el lado izquierdo de esta ecuación es triangular superior y el derecho triangular inferior deberán ser ambos diagonales. Luego, debe serlo también $\Lb^{-1}\tilde \Lb$. Por construcción, $\tilde\Lb_{i,i}=1=\Lb_{i,i}$, para todo $1\le i\le n$ . Debido a la primera igualdad se tiene en particular que
$\Lb^{-1}_{i,i}=1$. De aquí resulta
$\Lb^{-1}\tilde \Lb=\Ib$, es decir
$\Lb=\tilde \Lb$ y además $\Db=\Db^*$.
En particular se obtiene la factorización
$$
\Ab=\Lb\Db\Lb^*.
$$
Como además $\Ab$ es definida positiva, entonces $\Db>0$, pues para todo $\vb\neq 0$
$$
0<\vb^*\Ab\vb=\vb^*\Lb\Db\Lb^*\vb=
(\Lb^*\vb)^*\Db\Lb^*\vb,
$$
de donde, llamando $\wb=\Lb^*\vb$ y usando que $\Lb$ (y por ende $\Lb^*$) es invertible, resulta la definida positividad de $\Db$, lo que en este caso, por ser diagonal, equivale a $\Db>0$. Esto nos garantiza la existencia de la factorización de Cholesky:
$$
\Ab=\Lb\Db^{\frac{1}{2}}\Db^{\frac{1}{2}}\Lb^*=\Cb\Cb^*,
$$
donde $\Cb$ es triangular inferior con elementos diagonales positivos.

La construcción de $\Cb$ puede hacerse desde la factorización LU pero eso no se hace de ese modo porque es mas costoso y menos estable que el algoritmo siguiente:

\begin{itemize}
 \item poner codigo de Cholesky
 \item dar  la complejidad $\frac{1}{3}n^3$
 \item Comentar porque es estable-
\end{itemize}



\section{Ortogonalidad}

Como hemos mencionado, el método de eliminación se basa en triangulación por matrices triangulares (los multiplicadores).
El crecimiento de los  multiplicadores y de la matriz triangular superior resultante podría ser problemático.  Por esa razón lo ideal sería realizar operaciones con matrices de norma controlada (idealmente de norma 1). Esos algoritmos son posibles y se basan en ideas de ortogonalidad.



Dada una familia de vectores $\{\vb_1, \vb_2, ...,\vb_n\}$ linealmente independientes. La ortonormalización de Gram-Schmidt
es un algoritmo que produce una sucesión de vectores ortonormales  $\{\qb_1,\qb_2,...,\qb_n\}$ del siguiente modo

\begin{tcolorbox}[width=\linewidth/3]
{\bf for} j=1 {\bf to} n

$\vb_j=\ab_j$

{\bf for} i=1 to j-1

$r_{i,j}=\qb_i^*\ab_j$

 $\vb_j=\vb_j-r_{i.j}
\qb_i$

{\bf end}

$r_{j,j}=\|\vb_j\|_2$

$\qb_{j}=\vb_j/r_{j,j}$

{\bf end}
\end{tcolorbox}


Esta sucesión verifica,
\begin{itemize}
 \item $\{\qb_1,\qb_2,...,\qb_k\}$ es ortonormal.
 \item $\langle\qb_1,\qb_2,...,\qb_k\rangle=\langle\vb_1,\vb_2,..., \vb_k\rangle$ para todo $1\le k\le n$.
\end{itemize}

\begin{tcolorbox}
\begin{rem}
Gram-Schmidt es ampliamente utilizado que funciona generalmente muy bien pero es un algoritmo inestable. Informalmente, los problemas del algoritmo se evidencian cuando la familia de vectores $\{\vb_1, \vb_2, ...,\vb_n\}$ es \emph{casi} linealmente dependiente. En ese caso, es de esperar que en el ítem
b) puedan restarse cantidades muy similares generando pérdida de dígitos significativos.

Ejemplo numérico...!!!

Mas adelante veremos ideas alternativas -estables- para ortonormalizar familias de vectores.

\end{rem}

\end{tcolorbox}
Dada una matriz $\Ab\in \C^{m\times n}$, con columnas $\{\ab_1,\ab_2,\cdots, \ab_n\}$ linealmente independientes, podemos considerar los subespacios anidados generados por sus columnas:
$$
<\ab_1> \subseteq <\ab_1,\ab_2> \subseteq\cdots \subseteq <\ab_1,\ab_2,\cdots, \ab_k>.\footnote{Si las columnas son $l.i.$ las inclusiones son obviamente estrictas, pero no en el caso general.}
$$
En varias aplicaciones es de interés encontrar generadores \emph{ortonormales} de esos subespacios. Esto es, una colección
$\{\qb_1,\qb_2,\cdots ,\qb_k\}$,
tales que $\qb_i^*\qb_j=\delta_i^j$ y
$$
 \langle \ab_1,\ab_2,\cdots, \ab_l\rangle =
 \langle \qb_1,\qb_2,\cdots, \qb_l\rangle
$$
para todo $l$.
Matricialmente, y observando los subespacios anidados, es obvio que esto equivale a hallar una matriz $\Rb\in \Cnn$ triangular
superior
$$
\Rb=
\begin{pmatrix}
 r_{1,1}&r_{1,2}&r_{1,3}&\cdots&r_{1,n}\\
 0&r_{2,2}&r_{2,3}&\cdots&r_{2,n}\\
 0&0&r_{3,3}&\cdot&r_{3,n}\\
 \vdots&\vdots&\vdots&\vdots&\vdots\\
 0&0&0&0&r_{n,n}
\end{pmatrix}
$$
tal que
$$
\begin{pmatrix}
&|&&|&&|&\\
&|&&|&&|&\\
\ab_1&|&\ab_2&|&\cdots&|&\ab_n\\
&|&&|&&|&\\
&|&&|&&|&\\
\end{pmatrix}
=\begin{pmatrix}
&|&&|&&|&\\
&|&&|&&|&\\
\qb_1&|&\qb_2&|&\cdots&|&\qb_n\\
&|&&|&&|&\\
&|&&|&&|&\\
\end{pmatrix}
\begin{pmatrix}
 r_{1,1}&r_{1,2}&r_{1,3}&\cdots&r_{1,n}\\
 0&r_{2,2}&r_{2,3}&\cdots&r_{2,n}\\
 0&0&r_{3,3}&\cdot&r_{3,n}\\
 \vdots&\vdots&\vdots&\vdots&\vdots\\
 0&0&0&0&r_{n,n}
\end{pmatrix},
$$
de donde se observa que tanto los $\qb_i$ (elegidos ortonomales) como los $r_{i,j}$ pueden obtenerse desde las ecuaciones
\begin{eqnarray*}
\ab_1&=&r_{1,1}\qb_1\\
\ab_2&=&r_{1,2}\qb_1+r_{2,2}\qb_2\\
&\vdots&\\
\ab_n&=&r_{1,n}\qb_1+r_{2,n}\qb_2+\cdots r_{n,n}\qb_n\\
\end{eqnarray*}
por ejemplo, a través de Gram-Schmidt. Como hemos mencionado, sin embargo, ese algoritmo no es estable\footnote{Hay una versión modificada utlizando proyectores que no analizaremos aquí.} y suele utilizarse en cambio un método alternativo. Sin embargo para propósitos teóricos funciona perfectamente.

Notemos que en caso de que $\Qb$ resulte cuadrada ($\Qb\in \C^{n\times n}$), será una matriz unitaria (ortogonal en caso de ser una matriz real).

\section{Factorización QR}
\tcc
\begin{teo}
Dada $\Ab\in \C^{m\times n}$ es posible escribirla como producto de dos factores $\tilde \Qb\in \C^{m\times n}$ y $\Rb\in \C^{n\times n}$.
 \end{teo}
\etcc

\begin{proof}
 Si $\Ab$ tiene sus columnas $l.i.$
 la construcción se obtiene del algoritmo de Gram-Schmidt. En efecto, el algoritmo solo fallará cuando algún $\vb_j$ sea cero y no se pueda dividir por $r_{j,j}$. De hecho eso implicaría que en ese paso $\ab_j\in \langle\qb_1,\cdots\qb_{j-1}\rangle=\langle\ab_1,\cdots\ab_{j-1}\rangle$ lo que contradice el hecho de que las columnas de $\Ab$ sean $l.i.$.

 En el caso en el que las columnas de $\Ab$ no sean $l.i.$, el algoritmo fallará del modo descripto. De ser así, elegimos $\qb_j$, $l.i.$ con
 $\{\qb_1,\cdots\qb_{j-1}\}$ y continuamos con el procedimiento las veces que sea necesario.

 En ambos casos se obtiene
 $$
 \Ab=\Qb\Rb.
 $$

 Esta factorización suele llamarse \emph{reducida} debido a que la matriz
 $\Qb$ no es cuadrada. Sin embargo, en el caso  $m\ge n$ siempre será posible
 completar $\{\qb_1,\cdots\qb_n\}$ a una base de $\C^m$ y agregar ceros adecuadamente en la matriz $\Rb$ para matener la igualdad $\Ab=\Qb\Rb$ de modo siguiente
 $$
 \stackrel{
 \begin{pmatrix}
  &&&&\\
  &&&&\\
  &&&&\\
  &&\Ab&&\\
  &&&&\\
  &&&&\\
  &&&&\\
  &&&&
 \end{pmatrix}=}{m\times n}
  \stackrel{\begin{pmatrix}
  &&&&\\
  &&&&\\
  &&&&\\
  &&\Qb&&\\
  &&&&\\
  &&&&\\
  &&&&\\
  &&&&
  \end{pmatrix}}{m \times m }\stackrel{ \begin{pmatrix}
  r_{1,1}&r_{1,2}&r_{1,3}&\cdots&r_{1,n}\\
 0&r_{2,2}&r_{2,3}&\cdots&r_{2,n}\\
 0&0&r_{3,3}&\cdot&r_{3,n}\\
 \vdots&\vdots&\vdots&\vdots&\vdots\\
 0&0&0&0&r_{n,n}\\
 0&0&0&0&0\\
 \vdots&\vdots&\vdots&\vdots&\vdots\\
 0&0&0&0&0
 \end{pmatrix}}{\underbrace{m\times n}},
 $$
que se denomina factorización QR \emph{completa}.
 \end{proof}
 Consideremos la factorización reducida. Podemos multiplicar una columna arbitraria, digamos la columna $j$,  de $\Qb$ por un complejo $z$ de la forma $e^{ix}\neq 1$ y la fila $j$ por $z^{-1}=e^{-ix}$ obteniendo una nueva factorización QR reducida, el resultado siguiente dice que salvo esa modificación, la factorización es única.
\tcc
\begin{teo}
Dada $\Ab\in \C^{m\times n}$ con $m\ge n$ y rango máximo existe una única factorización reducida con la propiedad de que $\R \ni r_{i,i}>0$.
 \end{teo}
\etcc

\begin{proof}
 lll
\end{proof}


\begin{itemize}
 \item Poner complejidad de usar Gramm-Schmidt
 \item Householder y mencionar Givens
 \item Resolución de sistemas por QR
\end{itemize}