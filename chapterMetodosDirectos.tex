 
\chapter{Métodos Directos Para Sistemas Lineales}
\setcounter{equation}{0}

\section{Sistemas y Factorización de Matrices}

Comenzaremos asumiendo que trabajaremos con matrices cuadradas, i.e.  $\Ab\in \R^{n\times n}$ (o mas en general
$\K^{n\times n}$ ya que la mayoría de las consideranciones se aplican a los complejos).

Recordemos que los vectores $\vb\in \Kn$ los identificamos con matrices columna de $n\times 1$ de donde tiene sentido $\Ab\vb$.

Factorizar siginifica descomponer una expresión en factores de algún modo mas simples o que nos otorguen algún beneficio teórico o práctico. La factorización
LU consiste en escribir una matriz invertible $\Ab$ como producto de dos factores: uno triangular inferior $\Lb$ y otro triangular superior $\Ub$, es decir
$$
\Lb= \begin{pmatrix}
*& 0 & \cdots & 0 \\
* & * & \cdots & 0 \\
\vdots  & \vdots  & \ddots & \vdots  \\
* & * & \cdots & * 
\end{pmatrix}, 
\Ub= \begin{pmatrix}
*& * & \cdots & * \\
0 & * & \cdots & * \\
\vdots  & \vdots  & \ddots & \vdots  \\
0 & 0 & \cdots & * 
\end{pmatrix}
$$
y
$$
\Ab=\Lb\Ub.
$$
Un beneficio inmediato de esta factorización es que para  resolver un sistema
$$
\Ab\xb=\bb,
$$
basta resolver
$$
\Lb\yb=\bb,
$$
$$
\Ub\xb=\yb,
$$
y cada sistema triangular puede resolverse por sustitución (regresiva o progresiva) en $\sim n^2$ operaciones\footnote{De todos modos el costo de la factorización es de orden $\sim \frac23 n^3$ como veremos mas adelante.}.
\begin{ej} Escriba  un algoritmo para resolver sistemas triangulares con orden $n^2$.
\end{ej}

\section{Descomposición $\Lb\Ub=\Ab$}

La factorización $LU$ es un subproducto del método de eliminación de Gauss.
Dada la matriz
$$
\Ab= \begin{pmatrix}
a_{1,1}& a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n,1} & a_{n,2} & \cdots & a_{n,n}
\end{pmatrix}
$$
y suponiendo que $a_{1,1}$, denominado \emph{pivot}, es no nulo es posible operar
sobre las filas de $\Ab$ para generar ceros debajo del elemento
$a_{1,1}$. Esto se hace fila a fila multiplicando la fila 1 de $\Ab$ por $-\frac{a_{i,1}}{a_{1,1}}$ (lo cual es
posible ya que hemos asumido  $a_{1,1}\neq 0$) y sumando ese resultado a
la fila $i$. En términos matriciales, eso
equivale a multiplicar a izquierda la matriz $\Ab$ por el
multiplicador $\Mb_1$
$$
\Mb_1 =
\begin{pmatrix}
1 & 0 & \cdots & 0 \\
-\frac{a_{2,1}}{a_{1,1}} & 1 & \cdots & 0 \\
\vdots  & \vdots  & \ddots & \vdots  \\
-\frac{a_{n,1}}{a_{1,1}} & 0 & \cdots & 1
\end{pmatrix} $$
El efecto neto del producto resulta
$$
\Mb_1\Ab= \begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} &\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} &\cdots & a^{(1)}_{2,n} \\
0 & a^{(1)}_{3,2} & a^{(1)}_{3,3} & \cdots & a^{(1)}_{3,n} \\

\vdots  & \vdots  & \ddots & \vdots  \\
0 & a^{(1)}_{n,2}& a^{(1)}_{n,3} & \cdots & a^{(1)}_{n,n}
\end{pmatrix}
$$
donde se destaca con un superíndice $(1)$ una nueva matriz
$\Ab^{(1)}\in \R^{(n-1)\times (n-1)} $
$$
\Ab^{(1)}=
 \begin{pmatrix}
 a^{(1)}_{2,2} & \cdots & a^{(1)}_{2,n} \\
\vdots  & \vdots  & \ddots &   \\
a^{(1)}_{n,2} & \cdots & a^{(1)}_{n,n}
\end{pmatrix}
$$
que en caso de tener el pivot $a^{(1)}_{2,2}\neq 0$ admite un nuevo paso en la iteración tomando
$$
\Mb_2 =
\begin{pmatrix}
1 & 0 & \cdots & 0 & 0 \\
0 & 1& \cdots & 0 & 0 \\
0 & -\frac{a_{3,2}^{(1)}}{a^{(1)}_{2,2}} & 1 & \cdots & 0 \\
\vdots  & \vdots  & \vdots & \vdots  \\
0&-\frac{a_{n,2}^{(1)}}{a^{(1)}_{2,2}} & 0 & \cdots & 1
\end{pmatrix} $$
de donde
$$
\Mb_2\Mb_1\Ab= \begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} &\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} &\cdots & a^{(1)}_{2,n} \\
0 & 0 & a^{(2)}_{3,3} & \cdots & a^{(2)}_{3,n} \\

\vdots  & \vdots  & \ddots & \vdots  \\
0 & 0& a^{(2)}_{n,3} & \cdots & a^{(2)}_{n,n}
\end{pmatrix}
=\begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} &\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} &\cdots & a^{(1)}_{2,n} \\
0 & 0 & &  &  \\
\vdots  & \vdots &  & \Ab^{(2)} &   \\
0 & 0&  &
\end{pmatrix},
$$
con $\Ab^{(2)}\in \R^{(n-2)\times (n-2)}$.
Este procedimiento, dado que cada $\Mb_i$ es triangular, se denomina  \emph{triangulación por matrices triangulares}. Si algoritmo no se detiene, es decir que cada elemento pivot $a_{k+1,k+1}^{(k)}\neq 0$, para todo $1\le k\le n-1$, se obtiene una matriz triangular superior $\Ub$ invertible
$$
\Mb_{n-1}\Mb_{n-2}\cdots \Mb_{1}\Ab=\Ub=
\begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} & a_{1,4}&\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} & a^{(1)}_{2,4} &\cdots & a^{(1)}_{2,n} \\
0 &0 & a^{(2)}_{3,3} & a^{(2)}_{3,4}  &\cdots & a^{(2)}_{3,n} \\
%|0 & 0 & 0 &  & & \\
\vdots  & \vdots & \vdots  & \vdots  & \vdots  & \vdots   \\
0 & 0& 0 & 0& 0& a^{(n-1)}_{n,n}
\end{pmatrix}
$$
Observemos que la productoria
$$
\Mb_{n-1}\cdots\Mb_2\Mb_1=\Mb,
$$
es una matriz triangular inferior, dado que es producto de matrices triangulares inferiores. La inversa de $\Mb$, es por lo tanto triangular inferior. La llamaremos $\Lb=\Mb^{-1}$ y por ende
$$
\Ab=\Lb\Ub.
$$
El problema que aparece inmediatamente es que no hemos obtenido $\Lb$ constructivamente (en el sentido de que aún falta invertir $\Mb$). Sin embargo, un hecho notable facilita ese trabajo.

Observemos que cada $\Mb_i$ tiene la forma
\begin{equation}
 \label{eq:formaMi}
\Mb_i=\Ib-\zb_i\eb_i^T,
\end{equation}
donde
$\eb_i$ representa el $i-$ésimo canónico y
$\zb_i$ es de la forma
$$
\zb_i=
\begin{pmatrix}
 0\\
 \vdots\\
 0\\
 z_{i+1}^{(i)}\\
 \vdots \\
 z_n^{(i)}
\end{pmatrix},
$$
con $z_j^{(i)}=\frac{a^{(i-1)}_{j,i}}{a^{(i-1)}_{i,i}}$, $i+1\le j\le n$.

\begin{tcolorbox}[colback=black!15!white,colframe=black!75!black]
\begin{lema}\label{lema:inversamultiplicador}
 Sea $\ub,\vb\in \Rn$, tales que $\vb^T\ub\neq 1$ entonces $\Ib-\ub\vb^T$ es invertible y además
 $$(\Ib-\ub\vb^T)^{-1}=\Ib+\frac{\ub\vb^T}{1-\vb^T\ub}.$$
\end{lema}
\end{tcolorbox}
\begin{proof}
 $$(\Ib-\ub\vb^T)(\Ib+\frac{\ub\vb^T}{1-\vb^T\ub})=\Ib+\ub\vb^T(\frac{1}{1-\vb^T\ub}-1)-\frac{\ub\vb^T\ub\vb^T}{1-\vb^T\ub}=\Ib. $$
\end{proof}
Considerando el lema anterior y observando que en  \eqref{eq:formaMi} se tiene
$\eb_i^T\zb_i=0$, resulta \footnote{Es decir que el costo de invertir $\Mb_i$ es casi nulo (apenas un cambio de signos debajo de la diagonal).}
$$
\Mb_i^{-1}=\Ib+\zb_i\eb_i^T,
$$
de donde
$$
\Lb=\Mb_1^{-1}\Mb_2^{-1}\cdots \Mb_{(n-1)}^{-1}=(\Ib+\zb_1\eb_1^T)(\Ib+\zb_2\eb_2^T)\cdots (\Ib+\zb_{n-1}\eb_{n-1}^T).
$$
Ademas de la sencillez en el cómputo de $\Mb_i^{-1}$ hay otro hecho ``afortunado'' que permite expresar $\Lb$ sin cálculos adicionales. Como $\eb_i^T\zb_k=0$ no solo para $k=i$ sino para todo $i\le k\le n$,  se puede desarrollar la expresión para $\Lb$ y obtener
$$
\Lb=\Ib+\sum_{i=1}^{n-1}\zb_i\eb_i^T,
$$
o dicho de otra forma,
$$
\Lb=
\begin{pmatrix}
1 & 0 & \cdots & 0 & 0\\
\frac{a_{2,1}}{a_{1,1}} & 1 & \cdots & 0 & 0 \\
\frac{a_{3,1}}{a_{1,1}} & \frac{a_{3,2}^{(1)}}{a^{(1)}_{2,2}}
\vdots  & \vdots  & \ddots & \vdots  \\
\vdots  & \vdots  & \ddots &1&  \vdots  \\
\frac{a_{n,1}}{a_{1,1}} &  \frac{a_{n,2}^{(1)}}{a^{(1)}_{2,2}} & \cdots & \frac{a_{n,n-1}^{(n-1)}}{a^{(n-1)}_{n-1,n-1}}& 1
\end{pmatrix},
$$
i.e., basta con almacenar los multiplicadores en sus respectivos lugares cambiando los signos.




Un supuesto básico para que la descomposición LU pueda llevarse a cabo
es que todos los pivots sean no nulos.
Eso no puede garantizarse bajo la única hipótesis de que $det(\Ab)\neq 0$ como se ve en tomando por ejemplo
$$
\Ab=
\begin{pmatrix}
 0&1\\
 1&0
\end{pmatrix}
$$

Sin embargo puede garantizarse si se requiere que todos lo menores de la matriz $\Ab$,
sean invertibles.

\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black]
\begin{prop}\label{prop:menorenonulos}
 Sea $\Ab\in \Knn$, entonces existe factorización LU de $\Ab$ con $\Lb,\Ub$ \emph{invertibles} sí y solo sí
 para todo  $1\le k\le n$,  $\det(\Ab(1:k,1:k))\neq 0$.
\end{prop}

\end{tcolorbox}
 \begin{proof} Veamos primero la implicación $\Leftarrow\,$:
 
 
 Como $a_{11}=A(1,1)$ resulta $a_{11}\neq 0$ y el primer pivot es no nulo. LLevado adelante el proceso de eliminación, vemos que se puede escribir
$$
\Ab=\Mb_1^{-1}\Ub_1
$$
donde

$$
\Mb_1^{-1}\Ub_1=
\begin{pmatrix}
1 & 0 & \cdots & 0 & 0\\
\frac{a_{2,1}}{a_{1,1}} & 1 & \cdots & 0 & 0 \\
\frac{a_{3,1}}{a_{1,1}} & 0
\vdots  & \vdots  & \ddots & \vdots  \\
\vdots  & \vdots  & \ddots &1&  \vdots  \\
\frac{a_{n,1}}{a_{1,1}} &  0 & \cdots & 0& 1 
\end{pmatrix}\begin{pmatrix}
a_{1,1}& a_{1,2} & a_{1,3} &\cdots & a_{1,n} \\
0 & a^{(1)}_{2,2} & a^{(1)}_{2,3} &\cdots & a^{(1)}_{2,n} \\
0 & 0 & &  &  \\
\vdots  & \vdots &  & \Ab^{(2)} &   \\
0 & 0&  &  
\end{pmatrix}
$$
Por lo tanto
$$
\Ab(1:2,1:2)=\begin{pmatrix}
1 & 0 \\
\frac{a_{2,1}}{a_{1,1}} & 1\\
\end{pmatrix}\begin{pmatrix}
a_{1,1}& a_{1,2} \\
0 & a^{(1)}_{2,2} 
\end{pmatrix}
$$
y como por hip\'otesis
$0\neq det(\Ab(1:2,1:2))$ tenemos $det(\Ab(1:2,1:2))=a_{1,1}a_{2,2}^{(1)}\neq 0$, por lo tanto el segundo pivot no se anula. La demostración se sigue por inducción.

Para la implicación $\Rightarrow\,$: Notamos que si $\Ab=\Lb\Ub$ con $\Lb$ y $\Ub$ invertibles, deben ser $l_{kk}\neq 0 \neq u_{kk}$ para todo $1\le k \le n$ ya que son triangulares. En particular $\det(L(1:k,1:k)\neq 0 \neq \det(U(1:k,1:k))$ y por ser  una de ellas triangular inferior y la otra superior, resulta $A(1:k,1:k)=L(1:k,1:k)U(1:k,1:k)$ de donde
$\det(A(1:k,1:k))\neq 0$.
 \end{proof}

Para ver otro criterio necesitamos una definición.
\tccdefi
\begin{defi}
  \label{defi:deDD}
  Una matriz $\Ab\in \Rnn, \Cnn$ se dice \emph{diagonal dominante} (\emph{estrictamente diagonal dominante}) y se denota DD (EDD) si y solo si para todo $i$,
 $1\le i\le n$
 $$
 \sum_{1\le j \le n, j\neq i}|a_{i,j}|\le (<) |a_{i,i}|.
 $$

 \end{defi}
\end{tcolorbox}
Y la siguiente proposición (que puede refinarse en algunos casos como veremos mas adelante).
\tcc
\begin{prop}
 Sea $\Ab\in \Rnn, \Cnn$ EDD entonces $\Ab$ es invertible.
\end{prop}
\etcc
\begin{proof}
 Supongamos que no. Existe entonces
 ${\bf 0}\neq \vb\in\Cn$ tal que
 $\Ab\vb=\cero$. Sea $i$, $1\le i\le n$ tal que $0\neq |v_i|=\|\vb\|_\infty$, entonces
 $$
 \sum_{j=1}^na_{i,j}v_j=0,
 $$
 de donde
 $$
 |a_{i,i}|\le \sum_{j=1, j\neq i}^n|a_{i,j}|\frac{|v_j|}{|v_i|}\le
 \sum_{j=1, j\neq i}^n|a_{i,j}|,
 $$
 lo que contradice la EDD.
\end{proof}






\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black] \begin{prop}
  Si $\Ab$ es EDD entonces, trabajando con aritmética exacta, el proceso de eliminación de Gauss no produce pivots nulos.
 \end{prop}
\end{tcolorbox}
\begin{proof}
 Ejercicio.
\end{proof}

\begin{tcolorbox}
\begin{rem}
 Las matrices DD no son una artificio teórico, aparecen frecuentemente en la práctica como por ejemplo en la  discretización de ecuaciones diferenciales.
\end{rem}
\etcc

Aún en el caso en que no aparezcan pivots nulos, el algoritmo de eliminación no delvoverá en general los verdaderos factores $\Lb$ y $\Ub$, sino una aproximación de ellos. Mas explícitamente podemos enunciar el siguiente teorema (ver \cite{TB}).
\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black]
\begin{teo}\label{teo:errorLU}
 Sea $\Ab\in \Knn$,   si el algoritmo LU no produce pivots nulos,la factorización LU producida por la máquina verifica
 $$
 \tilde\Lb\tilde \Ub= \Ab + \Delta \Ab
 $$
donde $|\Delta\Ab|\lesssim 2(n-1)\varepsilon(| \Ab|+|\tilde \Lb||\tilde \Ub|).
$
\end{teo}

\end{tcolorbox}


\begin{tcolorbox}
\begin{rem}
 El Teorema \ref{teo:errorLU}, sugiere controlar el tamaño de los factores
$\tilde\Lb,\tilde\Ub$. En efecto, si el producto
$|\tilde \Lb||\tilde \Ub|$ es muy grande no tendremos control (al menos teórico) sobre el error. El \emph{pivoteo parcial} permite garantizar al menos el control $|\Lb|\le 1$ a una costo razonable. Si bien, esto no necesariamente implica que $|\tilde \Lb||\tilde \Ub|$ sea pequeño, resulta suficiente en la mayoría de los casos prácticos (ver sin embargo la Observación \ref{obs:malgauss}).

\end{rem}

\end{tcolorbox}


\begin{ej}
 Muestre el el número de operaciones necesarias para realizar la factorización LU es $\mathcal{O} (\frac{2n^3}{3})$
\end{ej}


El número de elementos de una matriz de $n\times n$ es obviamente $n^2$. El número mínimo de operaciones que un método de resolución puede efectuar debe ser de orden mayor o igual a $n^2$. Todos los métodos directos clásicos son de orden cúbico. Esto presenta problemas para matrices muy grandes ya que en ciertas aplicaciones es posible tener un gran número de incógnitas.

Otro problema importante es el \emph{rellenado} que produce el algoritmo. Esto es, aunque $\Ab$ sea \emph{rala} -cosa que afortunadamente ocurre en muchas aplicaciones, como en ecuaciones diferenciales- los factores $\Lb,\Ub$ pueden no serlo.

\begin{ej}
 Cuanta memoria ocuparía una matriz
 de $10^6\times 10^6$ llena?.
\end{ej}



\section{Descomposición $\Lb\Ub=\Pb\Ab$ (pivoteo parcial)}

Una matriz de permutaciones $\Pb\in \Knn$
es una matriz identidad con las filas permutadas. Alternativamente puede verse como una matriz cuyas filas están conformadas por los elementos de la base canónica no necesariamente ordenados. Es decir,
$$
\Pb=
\begin{pmatrix}
 \eb_{i_1}^T \\
 ---- \\
 \eb_{i_2}^T\\
 \vdots

 \\
 -----\\
 \eb_{i_n}^T
\end{pmatrix}
$$
donde $i_1,i_2,\cdots i_n\in \{1,2,\cdots n\}$ son todos diferentes. Alternativamente podemos pensar una matriz de permutaciones como la identidad con las columnas permutadas
$$
\Pb=
\begin{pmatrix}
 \eb_{i_1}&|& \eb_{i_2}&|&\cdots |\eb_{i_n}
\end{pmatrix}.
$$
Notemos que si
$$
\vb \in \Kn,
$$
$$
\Pb\vb=
\begin{pmatrix}
 v_{i_1}\\
 v_{i_2}\\
 \vdots \\
 v_{i_n}
\end{pmatrix}
$$
$$
\vb^T\Pb=
\begin{pmatrix}
 v_{i_1},
 v_{i_2},
 \cdots
 v_{i_n}
\end{pmatrix}.
$$
Resumamos las siguientes propiedades de las permutaciones.
\begin{itemize}
 \item Dada $\Ab\in \Knn$, $\Pb\Ab$ coincide con $\Ab$ pero con las filas permutadas y $\Ab\Pb$ coincide con $\Ab$ pero con las columnas permutadas.
 \item Dada $\xb\in \Kn$, $\Pb\xb$ coincide con $\xb$ pero con los elementos permutados.
 \item $\det(\Pb)=\pm 1$
 \item $\Pb^{-1}=\Pb^T$
 \item Si $\Pb_1$ y $\Pb_2$ son permutaciones entonces $\Pb=\Pb_1\Pb_2$ también lo es.
\end{itemize}


Si durante la eliminación de Gauss hallamos un pivot nulo, podemos intercambiar filas para continuar con el algoritmo (siempre que haya algún elemento no nulo con el cual proseguir). Mas aún, aunque el correspondiente pivot sea no nulo, podemos,  casi al mismo costo, tomar el pivot mas grande (en valor absoluto).  Esa operación la podemos representar matricialmente multiplicando por una adecuada permutación.

El procedimiento sería el siguiente: tomemos el elemento con mayor valor absoluto en la primera columna de $\Ab$. Digamos que este es $a_{k,1}$ (si $\det(\Ab)\neq 0$ se tiene que $a_{k,1}\neq 0$). Permutemos la fila $k$ con la fila $1$, lo cual se hace con una matriz $\Pb_1$. Es decir
$$
\Pb_1\Ab=
\begin{pmatrix}
 a_{k,1}&a_{k,2}&\cdots &a_{k,n}\\
 a_{2,1}&a_{2,2}&\cdots &a_{2,n}\\
  \vdots&\vdots&\vdots&\vdots\\
 a_{1,1}&a_{1,2}&\cdots &a_{1,n}\\
 \vdots&\vdots&\vdots&\vdots \\
 a_{n,1}&a_{n,2}&\cdots &a_{n,n}
\end{pmatrix},
$$
a partir de aquí construimos el multiplicador, $\Mb_1$ de modo tal que
$$
\Mb_1\Pb_1\Ab=
\begin{pmatrix}
 a_{k,1}&a_{k,2}&\cdots &a_{k,n}\\
 0&&&\\
  \vdots&&\Ab^{(1)}&\\
 0&&&\\

\end{pmatrix}.
$$
Llegado a este punto, repetimos el procedimiento con la primer columna de $\Ab^{(1)}$ que eventualmente requerirá otra matriz $\Pb_2$ antes de la construcción del nuevo multiplicador $\Mb_2$. En definitiva,
\begin{equation}
 \label{eq:factoresPLU}
\Mb_{n-1}\Pb_{n-1}\cdots \Mb_{1}\Pb_{1}\Ab=\Ub,
 \end{equation}


se obtiene una nueva matriz triangular superior $\Ub$, y donde
$\Mb_i=\Ib-\zb_i\eb_i^T$ con
$$
\zb_i=
\begin{pmatrix}
 0\\
 \vdots\\
 0\\
 z_{i+1}^{(i)}\\
 \vdots \\
 z_n^{(i)}
\end{pmatrix},
$$


$|\zb_i|\le 1$ (i.e. los multplicadores son menores o iguales a 1 en valor absoluto).
El problema ahora es que en la forma \eqref{eq:factoresPLU}  no parece verse el modo de reconstruir los factores $\Lb$ y $\Pb$ de manera sencilla. Sin embargo un hecho inesperadamente favorable resuelve esta cuestión. Notemos que por el orden de las iteraciones en el paso $k$ solo permutaremos en busca del mayor pivot (en caso de ser necesario) filas desde la $k$ a la $n$ porque las filas previas ya han sido procesadas. Eso indica que
$\eb_j^T\Pb_k=\eb^T_j$ para todo $j<k$. En particular, si bien \emph{no es cierto} que $\Pb_k$
conmuta con $\Mb_j$ sí es cierto,
\emph{para todo $j<k$}, que
$$
\Pb_k\Mb_j=\Pb_k
(\Ib-\zb_{j}\eb_{j}^T)=
(\Ib-\Pb_k\zb_{j}\eb_{j}^T)\Pb_k=
\tilde\Mb_j \Pb_k,
$$
es decir que podemos pasar la permutación a la derecha permutando adecuadamente los multiplicadores de la matriz $\Mb_j$. Lo importante es que esta nueva matriz, denotada con $\tilde \Mb_j$ tiene la misma estructura que $\Mb_j$. Así que volviendo a  \eqref{eq:factoresPLU} tenemos
$$
\Mb_{n-1}\Pb_{n-1}\cdots \Mb_{1}\Pb_{1}\Ab= \tilde \Mb_{n-1}\cdots \tilde \Mb_{1} \Pb_{n-1}\cdots \Pb_{1}\Ab=\Ub.
$$
Argumentando como en el caso son pivoteo (usando que la estructura de las $\tilde \Mb_i$ es igual a la de las de las $\Mb_i$) y llamando
$\Pb=\Pb_{n-1}\cdots \Pb_{1}$ se llega a
$$
\Pb\Ab=\Lb\Ub.
$$

\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black] \begin{prop}
  Trabajando con aritmética exacta se tiene que si $det(\Ab)\neq 0$ el algoritmo de pivoteo parcial no tiene pivots nulos.
 \end{prop}
\end{tcolorbox}

\begin{tcolorbox}
[colback=black!15!white,colframe=black!75!black] \begin{rem}
\label{obs:malgauss}
El proceso de pivoteo parcial garantiza que $|\Lb|\le 1$ pero no
se  tiene un control demasiado beningno sobre el tamaño de
$|\Ub|$, como se ve en este ejemplo
$$
\begin{pmatrix}
 1&0&0&\cdots&0&1\\
 -1&1&0&\cdots&0&1\\
 -1&0&1&0&\cdots&1 \\
 \vdots &\vdots&\vdots& \ddots&&\vdots\\
 -1&0&0&0&\cdots&1
\end{pmatrix},
$$
puede verse que $\|\Ub\|_\infty \ge 2^n\|\Ab\|_\infty$.
Este ejemplo sin embargo parece ser singular en el sentido de que en los casos genéricos el crecimiento de $|\Ub|$ estaría controlado por $\sqrt{n}$ \cite{TB} \footnote{Al presente no parece haber una demostración rigurosa de esto.}. Esta particularidad parece (junto con el hecho de que su costo es la mitad que el de la factorización $QR$ que veremos mas adelante) haber mantenido el algoritmo de eliminación como uno de los más populares entre los métodos directos.
 \end{rem}
\end{tcolorbox}

Controlar el tamaño del ambos factores $\Lb$ y $\Ub$ es posible.
La idea se denomina \emph{pivoteo completo}, esto implica hacer permutaciones de filas y de columnas para tomar siempre el mayor pivot posible. Así en el paso inicial se busca el elemento $a_{k,l}$ con máximo valor absoluto y se permutan la fila y columna 1 con la fila y columna $k$ y $l$ respectivamente.  En términos matriciales eso requiere de dos matrices de permutaciones $\Pb_1,\tilde \Pb_1$ antes de aplicar el paso de eliminación. Es decir
$$
\Mb_1\Pb_1\Ab\tilde\Pb_1=
\begin{pmatrix}
 *&*&\cdots &*\\
 0&&&\\
  \vdots&&\Ab^{(1)}&\\
 0&&&\\

\end{pmatrix}.
$$
Repitiendo el procedimiento se obtiene $\Ub$
$$
\Mb_{n-1}\Pb_{n-1}\cdots \Mb_1\Pb_1\Ab\tilde\Pb_1\cdots \tilde\Pb_{n-1}=\Ub,
$$
y argumentando como antes se obtienen ahora dos matrices de permutaciones $\Pb,\tilde\Pb$ tales que
$$
\Pb\Ab\tilde\Pb=\Lb\Ub.
$$
El pivoteo completo no se lleva a cabo en general debido a su alto costo (calcule el número de comparaciones que debe realizar en cada) ya que hay otros métodos estables mas económicos. Wilkinson probó lo siguiente
\tcc
\begin{teo}
 En aritmética exacta, para el algoritmo de pivoteo completo se tiene
 $$
 \|\Ub\|_{\infty}\le \sqrt{n}(2.3^{1/2}.4^{1/3}\cdots n^{1/(n-1)})^{1/2}\|\Ab\|_\infty
 $$
\end{teo}
\etcc
\begin{ej}
 Estimar numéricamente el crecimiento del factor
 $\sqrt{n}(2.3^{1/2}.4^{1/3}\cdots n^{1/(n-1)})^{1/2}$.
\end{ej}

\section{Descomposición $\Lb \Db\Lb^*$ : Cholesky}


En muchos casos es posible explotar la estructura de las matrices a la hora de resolver sistemas. Dicho sea esto a la hora de ahorrar computos o mejorar la estabilidad de los algoritmos.


\begin{prop}
\label{prop:defposesinvert}
Si $\Ab$ es definida positiva entonces es invertible y también lo son todos sus menores.
\end{prop}
\begin{proof}
 En efecto, de no ser invertible, existiría $\cero\neq\vb$ tal que
 $\Ab\vb=\cero$ lo que contradice la definida positividad ya que para ese $\vb$ se tendría $\vb^*\Ab\vb=0$.

 Respecto de los menores el resultado se sigue del anterior. En efecto, sea $k$ fijo, $1\le k\le n$ y tomemos
 vectores $\vb\in \K^n$ de la forma
 $(\tilde \vb,0,\cdots,0)$ con $\tilde \vb \in \K^k$ arbitrario. Resulta
 $$
 0<\vb^*\Ab\vb=\tilde\vb^*\Ab(1:k,1:k)\tilde \vb,
 $$
 y de ahí la definida positividad de $\Ab(1:k,1:k)$ y por ende su invertibilidad.
\end{proof}


Si $\Ab$ es definida positiva  admite una descomposición $\Ab=\Lb\Ub$ (gracias a las Proposiciones \ref{prop:defposesinvert} y \ref{prop:menorenonulos}) y
podemos escribir  $\Ub=\Db\tilde \Lb^*$, con $\tilde \Lb$ triangular inferior con $1$ en la diagonal y $\Db$ una matriz diagonal. De ahí resulta
$$
\Ab=\Lb\Db\tilde{ \Lb}^*=\tilde \Lb\Db^*\Lb^*,
$$
entonces
$$
\Db(\Lb^{-1}\tilde{ \Lb})^* = \Lb^{-1}\tilde \Lb\Db^*,
$$
y como el lado izquierdo de esta ecuación es triangular superior y el derecho triangular inferior deberán ser ambos diagonales. Luego, debe serlo también $\Lb^{-1}\tilde \Lb$. Por  construcción, $\tilde\Lb_{i,i}=1=\Lb_{i,i}$, para todo $1\le i\le n$ . Debido a la primera igualdad se tiene en particular que
$\Lb^{-1}_{i,i}=1$. De aquí resulta
$\Lb^{-1}\tilde \Lb=\Ib$, es decir
$\Lb=\tilde \Lb$ y además $\Db=\Db^*$.
En particular se obtiene la factorización
$$
\Ab=\Lb\Db\Lb^*.
$$
Como además $\Ab$ es definida positiva, entonces $\Db>0$, pues para todo $\vb\neq 0$
$$
0<\vb^*\Ab\vb=\vb^*\Lb\Db\Lb^*\vb=
(\Lb^*\vb)^*\Db\Lb^*\vb,
$$
de donde, llamando $\wb=\Lb^*\vb$ y usando que $\Lb$ (y por ende $\Lb^*$) es invertible, resulta la definida positividad de $\Db$, lo que en este caso, por ser diagonal, equivale a $\Db>0$. Esto nos garantiza la existencia de la factorización de Cholesky:
$$
\Ab=\Lb\Db^{\frac{1}{2}}\Db^{\frac{1}{2}}\Lb^*=\Cb\Cb^*,
$$
donde $\Cb$ es triangular inferior con elementos diagonales positivos. La construcción de $\Cb$ puede hacerse desde la factorización LU, sin embargo
ese modo mas costoso y menos estable que el algoritmo que describiremos debajo. Antes, sin embargo, veamos cómo sería la base de un algoritmo (garantizando que pueda ejecutarse sin interrupciones).

Asociemos el algoritmo con una función $chol()$\footnote{En Python $np.linalg.cholesky()$ en Matlab $chol()$.}. Es decir $chol(\Ab)$ devuelve el factor $\Cb$.

Ya que $\Ab=\Cb\Cb^*$, con $\Cb$ triangular inferior $c_{i,i}>0$, (de hecho, ya probamos la existencia de esta factorización). Intentaremos ``despejar'' $\Cb$.

Escribimos
$$
\Cb=
\begin{pmatrix}
 c_{1,1}&|&\cero \\
 -&-&-\\
 \Cb(2:n,1)&|&\Cb(2:n,2:n)
\end{pmatrix}
$$
donde $\Cb(2:n,2:n)$ tiene las mismas características que $\Cb$. Sabiendo que
\begin{eqnarray}
\label{eq:desc_de_A_:w
Chol}
\begin{pmatrix}
 a_{1,1}&|&\Ab(2:n,1)^* \\
 -&-&-\\
 \Ab(2:n,1)&|&\Ab(2:n,2:n)
\end{pmatrix}&= \\ \nonumber
\begin{pmatrix}
 c_{1,1}&|&\cero \\
 -&-&-\\
 \Cb(2:n,1)&|&\Cb(2:n,2:n)
\end{pmatrix}
&
\begin{pmatrix}
 c_{1,1}&|&\Cb(2:n,1)^* \\
 -&-&-\\
 \cero&|&\Cb(2:n,2:n)^*
\end{pmatrix}
\end{eqnarray}
se tiene, respectivamente, que
$$
\mbox{\bf Paso 1}  \qquad c_{1,1}^2=a_{1,1} \qquad \to \qquad c_{1,1}:=\sqrt{a_{1,1}},
$$

$$
\mbox{\bf Paso 2} \ \  \Ab(2:n,1)=c_{1,1}\Cb(2:n,1)\qquad\to\qquad \Cb(2:n,1):=\Ab(2:n,1)/c_{1,1},
$$

lo cual nos permitió ``despejar'' las incógnitas $\Cb(1:n,1)$, puesto que
$a_{1,1}>0$ por hipótesis, lo que permite elegir $c_{1,1}>0$. Ahora consideramos
$$
\mbox{\bf Paso 3} \ \ \Ab(2:n,2:n)=\Cb(1:n,1)\Cb(1:n,1)^*+\Cb(2:n,2:n)\Cb(2:n,2:n)^* \to $$
$$\Cb(2:n,2:n):= chol(\Ab(2:n,2:n)-\Cb(1:n,1)\Cb(1:n,1)^*),
$$
de donde el problema de resolver
$chol(\Ab(1:n,1:n))$ para una matriz de $n\times n$  se redujo
a resolver $chol(\Ab(2:n,2:n)-\Cb(2:n,1)\Cb(2:n,1)^*)$ para una matriz de tamaño $(n-1)\times (n-1)$, lo que inductivamente (siempre que la nueva matriz sea definida positiva) nos lleva al problema trivial de factorizar un número real positivo (matriz de $1\times 1$ definida positiva) como el cuadrado de otro positivo. Veamos entonces la siguiente
\tcc
\begin{prop}
 Si $\Ab$ es SDP entonces $\Ab(2:n,2:n)-\Cb(2:n,1)\Cb(2:n,1)^*$, donde
 $\Cb=(2:n,1)=\Ab(2:n,1)/\sqrt{a_{1,1}}$, es DP.
\end{prop}
\etcc
\begin{proof}
Claramente $\Ab(2:n,2:n)-\Cb(2:n,1)\Cb(2:n,1)^*$ es Hermitiana (o simétrica, en el caso real). Por otro lado, y por hipótesis, para todo $\cero\neq \ub=(u_1,\vb)\in \Cn$ se tiene
 $$
 0<\ub^*\Ab\ub=
 \begin{pmatrix}
  u_1\\
  -\\
  \vb
 \end{pmatrix}^*
\begin{pmatrix}
 a_{1,1}&|&\Ab(2:n,1)^* \\
 -&-&-\\
 \Ab(2:n,1)&|&\Ab(2:n,2:n)
\end{pmatrix}
 \begin{pmatrix}
  u_1\\
  -\\
  \vb
 \end{pmatrix}=
 $$
 $$
 |u_1|^2 a_{1,1}+u_1^{*}\Ab(2:n,1)^{*}\vb +u_1\vb^*\Ab(2:n,1)+\vb^*\Ab(2:n,2:n)\vb.
 $$
 Elijamos $u_1=-\frac{\Ab(2:n,1)^*\vb}{a_{1,1}}$ y resulta para todo $\vb\neq \cero$,
 $$
 0<\vb^*\left( \Ab(2:n,2:n)-\Cb(2:n,1)\Cb(2:n,1)^*\right)\vb,
 $$
 lo que completa la demostración.
\end{proof}
En forma de algoritmo, Cholesky puede
escribirse del siguiente modo,

{\bf for} j= 1:n

\qquad $a_{j,j}:=\sqrt{a_{j,j}}$

\qquad {\bf for} i=j+1:n

\qquad \qquad $a_{i,j}:=a_{i,j}/a_{j,j}$


\qquad {\bf for} k=j+1:n

\qquad\qquad\qquad  {\bf for} i=k:n

\qquad\qquad\qquad    $a_{i,k}:=a_{i,k}-a_{i,j}a_{k,j}$

en donde se utliliza solo la información de la parte triangular  inferior de $\Ab$ (la parte superior es redundante) y en donde, además, se almacenan los datos de la matriz $\Cb$.


\section{Ortogonalidad en Métodos Directos}

\begin{tcolorbox}
Como hemos mencionado, el método de eliminación se basa en triangulación por matrices triangulares (los multiplicadores).
El crecimiento de los  multiplicadores y de la matriz triangular superior resultante podría ser problemático.  Por esa razón lo ideal sería realizar operaciones con matrices de norma controlada (idealmente de norma 1). Esos algoritmos son posibles y se basan en ideas de ortogonalidad.
\end{tcolorbox}


Dada una matriz $\Ab\in \K^{m\times n}$, con columnas $\{\ab_1,\ab_2,\cdots, \ab_n\}$ linealmente independientes, podemos considerar los subespacios anidados generados por sus columnas:
$$
<\ab_1> \subset <\ab_1,\ab_2> \subset\cdots \subset <\ab_1,\ab_2,\cdots, \ab_k>.\footnote{Si las columnas son $l.i.$ las inclusiones son obviamente estrictas, pero no en el caso general.}
$$
En varias aplicaciones es de interés encontrar generadores \emph{ortonormales} de esos subespacios. Esto es, una colección
$\{\qb_1,\qb_2,\cdots ,\qb_k\}$,
tales que $\qb_i^*\qb_j=\delta_i^j$ y
$$
 \langle \ab_1,\ab_2,\cdots, \ab_l\rangle =
 \langle \qb_1,\qb_2,\cdots, \qb_l\rangle
$$
para todo $l$.




Matricialmente, y observando los subespacios anidados, es obvio que esto equivale a hallar una matriz $\Rb\in \Cnn$ triangular
superior
$$
\Rb=
\begin{pmatrix}
 r_{1,1}&r_{1,2}&r_{1,3}&\cdots&r_{1,n}\\
 0&r_{2,2}&r_{2,3}&\cdots&r_{2,n}\\
 0&0&r_{3,3}&\cdot&r_{3,n}\\
 \vdots&\vdots&\vdots&\vdots&\vdots\\
 0&0&0&0&r_{n,n}
\end{pmatrix}
$$
tal que
$$
\begin{pmatrix}
&|&&|&&|&\\
&|&&|&&|&\\
\ab_1&|&\ab_2&|&\cdots&|&\ab_n\\
&|&&|&&|&\\
&|&&|&&|&\\
\end{pmatrix}
=\begin{pmatrix}
&|&&|&&|&\\
&|&&|&&|&\\
\qb_1&|&\qb_2&|&\cdots&|&\qb_n\\
&|&&|&&|&\\
&|&&|&&|&\\
\end{pmatrix}
\begin{pmatrix}
 r_{1,1}&r_{1,2}&r_{1,3}&\cdots&r_{1,n}\\
 0&r_{2,2}&r_{2,3}&\cdots&r_{2,n}\\
 0&0&r_{3,3}&\cdot&r_{3,n}\\
 \vdots&\vdots&\vdots&\vdots&\vdots\\
 0&0&0&0&r_{n,n}
\end{pmatrix},
$$
de donde se observa que tanto los $\qb_i$ (elegidos ortonomales) como los $r_{i,j}$ pueden obtenerse desde las ecuaciones
\begin{eqnarray*}
\ab_1&=&r_{1,1}\qb_1\\
\ab_2&=&r_{1,2}\qb_1+r_{2,2}\qb_2\\
&\vdots&\\
\ab_n&=&r_{1,n}\qb_1+r_{2,n}\qb_2+\cdots r_{n,n}\qb_n\\
\end{eqnarray*}
por ejemplo, a través de Gramm-Schmidt. Notemos que en caso de que $\Ab$ resulte cuadrada ($\Ab\in \K^{n\times n}$), tendremos que $\Qb$ será una matriz unitaria (ortogonal en caso de ser una matriz real).
En particular, a través de Gramm-Schmidt hemos conluído que toda matriz $\Ab\in \K^{n\times n}$ invertible (en particular con todas sus columnas l.i.) puede factorizarse como una ortogonal $\Qb$ por una triangular superior $\Rb$
$$
\Ab=\Qb\Rb,
$$
factorización que se denomina QR.
